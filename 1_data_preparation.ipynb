{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Determining Sample Size\n",
    "\n",
    "In 3.1 of Hu, Wang, and Wu's paper on applying stylometric analysis to Dream of the Red Chamber, the first 60 chapters are randomly split into 80 samples. A google search on the character count of the novel shows the whole novel has 788451 characters, with the first 80 chapters taking up 607342. First, assuming rough uniformity in chapter length across the novel, the paper's approach to sample size would be akin to splitting the whole novel into 160 samples, for a sample size of about 4928 characters per sample. Next, we know from the google search result that the chapter lengths are indeed not uniform, with the first 80 chapters taking up 77% of the character count. Still using the same approach of splitting $x$ chapters into $\\frac{4}{3}x$, we have a sample size of 4270 characters per sample. The sample size is only supposed to be a rough range as we do not want to cut off novels mid-paragraph to create exact-size samples. Based on these results, 4500 seems like an acceptable cutoff. \n",
    "\n",
    "### Segmentation Method\n",
    "\n",
    "One way to segment the samples would be to write samples as the works are being parsed, and cut off samples when a word counter indicates the cutoff has been reached (taking care not to cut in the middle of paragraphs). However there is a chance this would result in leaving a small amount of text at the end too small to constitute a sample.\n",
    "\n",
    "Since we want to preserve whole paragraphs at all costs (paragraph attributes may be important), this limits using more dynamic ways to segment the samples and avoid undersised samples. Two remediation measures are proposed that while not perfect, may help prevent and/or \"beef up\" undersized samples.\n",
    "\n",
    "1. Instead of segmenting samples while going through an author's various works, we will first consolidate all of their works and tally up total character count $(X)$. Then using the preliminary cutoff $(n)$, decided to be 4500 earlier, determine how many samples this would produce. Rounding this preliminary number of samples we again divide total character count to get the real cutoff to use for this author's works. It is demonstrated in **Appendix A** that this measure performs extraordinary well in allowing the final sample to be either large enough to stand on its own, or small enough to merge into a previous sample. The rounding function we will use, denoted $r$, will be numpy's around function, which follows standard rounding procedure and has a preference for even numbers when given floats ending in \".5\". In summary, the real cutoff $(RC)$ will be determined by:\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "$$\n",
    "RC = r\\left(\\frac{X}{r(X/n)}\\right)\n",
    "$$\n",
    "\n",
    "2. A constant $k \\in (0,1)$ will be determined to work in conjunction with measure 1. If the leftover sample's character count exceeds $kn$, it will be a standalone sample, otherwise it will be merged into the previous sample. For this exercise, we arbitrarily choose $k$ to be $\\frac{1}{2}$, which **Appendix A** demonstrates is sufficient.\n",
    "\n",
    "###  Sample Format and Additional Requirements\n",
    "\n",
    "In addition to the size of each sample being fixed to some range, the sampler will also need to complete or account for the following: \n",
    "\n",
    "- Samples cannot end in the middle of paragraphs\n",
    "- Exactly 1 paragraph per line\n",
    "- Only 1 whitespace character, 1 newline, between paragraphs\n",
    "- Remove all heading and date lines\n",
    "\n",
    "### Taking a Look at the Data\n",
    "\n",
    "After looking through the novels downloaded from xstt5.com, it was found the following traits needed to be handled to comply with the listed requirements:\n",
    "\n",
    "- Text at the start and end of every novel about the site itself (It was found all these lines include variations \"txt\", such as \"TXT\" and \"t/xt\". Since it should be highly unlikely for these characters to show up in a 20th century Chinese novel, characters \"T\" and \"t\" will be used for the sampler to recognize these lines)\n",
    "- Certain lines were used for section titles and dates. All of these were unneccesary (The idea of taking these lines out will be based on the fact that they do not contain punctuation, apart from guillemets, brackets, and half commas)\n",
    "- For some novels, each paragraph of the actual novel content starts with 2-4 spaces, which need to be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths\n",
    "dpath = os.getcwd() # path to project directory\n",
    "\n",
    "wdname = \"/works\" # directory containing works\n",
    "wdpath = dpath + wdname # path to works directory \n",
    "\n",
    "sdname = \"/samples\" # directory to store samples\n",
    "sdpath = dpath + sdname # path to samples directory\n",
    "\n",
    "# get work names\n",
    "works = [w for w in os.listdir(wdpath) if op.isfile(op.join(wdpath, w))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and valid punctuation\n",
    "\n",
    "n = 4500\n",
    "k = 1/2\n",
    "valid_punc = ['。','！','？','“'] # chinese punctuation indicating paragraph validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def contains_valid_punc(para):\n",
    "    \"\"\"\n",
    "    Checks para for any valid punctation from valid_punc\n",
    "    \n",
    "    Parameter:\n",
    "        para: string of one paragraph\n",
    "    Returns:\n",
    "        True if para contains at least one valid punctuation char\n",
    "    \"\"\"\n",
    "    \n",
    "    for p in valid_punc:\n",
    "        if p in para:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_paragraphs(paras):\n",
    "    \"\"\"\n",
    "    Cleans paras by removing invalid paragraphs and whitespace in paragraphs\n",
    "    \n",
    "    Parameter:\n",
    "        paras: list of paragraph strings\n",
    "        \n",
    "    Returns:\n",
    "        paras: list of cleaned paragraph strings\n",
    "    \"\"\"\n",
    "    \n",
    "    paras = list(filter(contains_valid_punc, paras))\n",
    "    paras = [p for p in paras if \"T\" not in p and \"t\" not in p]\n",
    "    paras = [''.join(p.split()) for p in paras] # remove all whitespace\n",
    "    paras = [p for p in paras if p] # remove all now empty strings\n",
    "    return paras\n",
    "\n",
    "def real_cut(x, init_samsize):\n",
    "    \"\"\"\n",
    "    Calculates better sample size to use for segmenting samples\n",
    "    \n",
    "    Parameters:\n",
    "        x: character count of string to be segmented\n",
    "        init_samsize: initial sample size to be modified\n",
    "        \n",
    "    Returns:\n",
    "        Better sample size\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.around(x/np.around(x/init_samsize))\n",
    "\n",
    "def divide_samples(mass, au):\n",
    "    \"\"\"\n",
    "    Divides mass into samples and stores each sample into samples directory\n",
    "    \n",
    "    Parameters:\n",
    "        mass: list of paragraph strings\n",
    "        au: string of author name\n",
    "        \n",
    "    Returns:\n",
    "        rc: rough sample size used as calculated using real_cut\n",
    "        sc: number of samples created\n",
    "    \"\"\"\n",
    "    \n",
    "    x = sum(len(p) for p in mass)\n",
    "    rc = real_cut(x, n)\n",
    "    cc = 0 # sample character counter\n",
    "    sc = 0 # author sample counter\n",
    "    cur = [] # holds sample until dump\n",
    "    \n",
    "    if len(mass) <= 1:\n",
    "        quit()\n",
    "    \n",
    "    for p in mass:\n",
    "        cur.append(p)\n",
    "        cc += len(p)\n",
    "        if cc >= n:\n",
    "            sc += 1\n",
    "            sam = sdpath + '/' + \"{0}-{1}.txt\".format(au, str(sc))\n",
    "            with open(sam, 'w+', encoding=\"utf8\") as f:\n",
    "                for para in cur:\n",
    "                    f.write(\"{0}\\n\".format(para))\n",
    "            cc = 0\n",
    "            cur = []\n",
    "        elif p == mass[-1]: # apply remediation when reaching end of mass\n",
    "            if cc < k*n:\n",
    "                with open(sam, 'a', encoding=\"utf8\") as f:\n",
    "                    for para in cur:\n",
    "                        f.write(\"{0}\\n\".format(para))\n",
    "            else:\n",
    "                sc += 1\n",
    "                sam = sdpath + '/' + \"{0}-{1}.txt\".format(au, str(sc))\n",
    "                with open(sam, 'w+', encoding=\"utf8\") as f:\n",
    "                    for para in cur:\n",
    "                        f.write(\"{0}\\n\".format(para))\n",
    "    return [rc, sc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works are arranged by author in the directory\n",
    "\n",
    "allres = [] # to hold all sampling metric results\n",
    "mass = [] # to hold all an author's works\n",
    "curau = works[0].split('-')[0] # current author whose works mass holds\n",
    "\n",
    "for w in works:\n",
    "    au = w.split('-')[0] # get potentially new author name\n",
    "    if au != curau:\n",
    "        res = divide_samples(mass, curau)\n",
    "        allres.append([curau] + res)\n",
    "        mass = []\n",
    "        curau = au\n",
    "    fpath = wdpath + '/' + w\n",
    "    f = open(fpath, \"r\", encoding=\"utf8\")\n",
    "    paras = f.readlines()\n",
    "    paras = clean_paragraphs(paras)\n",
    "    mass = mass + paras\n",
    "\n",
    "res = divide_samples(mass, curau)\n",
    "allres.append([curau] + res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author      Rough Sample Size    Number of Samples\n",
      "--------  -------------------  -------------------\n",
      "余华                     4490                   77\n",
      "冯骥才                   4589                   19\n",
      "古龙                     4508                  189\n",
      "巴金                     4490                  181\n",
      "张爱玲                   4509                   70\n",
      "汪曾祺                   4519                   59\n",
      "沈从文                   4522                   65\n",
      "王安忆                   4496                   51\n",
      "矛盾                     4511                   64\n",
      "老舍                     4510                  199\n",
      "莫言                     4496                   57\n",
      "路遥                     4506                  173\n",
      "金庸                     4501                  723\n",
      "钱钟书                   4537                   51\n",
      "陈忠实                   4520                   96\n",
      "鲁迅                     4539                   51\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(allres, headers=['Author', 'Rough Sample Size', 'Number of Samples']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Cap for Number of Samples Per Author\n",
    "\n",
    "After generating the first iteration of samples, it was noted that the large differences in number of samples between authors would skewer features such as frequent words going forward. So a **hard cap of 70 samples per author** will be set for when features are extracted to form training and testing sets for the project. \n",
    "\n",
    "Additionally due to Feng Jicai's (冯骥才) collection of novels being to short to generate at least 50 samples, he will be excluded from the model building. His samples can be used after the models have been built to explore closeness in style. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
